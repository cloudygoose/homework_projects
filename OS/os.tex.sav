\documentclass[10pt]{article}
\usepackage{ctex}
\usepackage{times}
\usepackage{multicol}
%\usepackage{ftnright}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{nccfoots}
\geometry{left=1cm, right=1cm, top=2.5cm, bottom=2.5cm}

\begin{document}
\heiti
\begin{center}
\begin{huge}

\bf{Detecting and Surviving Data Races
using Complementary Schedules}

\end{huge}
\end{center}

\begin{center}
\begin{large}
Kaushik Veeraraghavan, Peter M. Chen, Jason Flinn, and Satish Narayanasamy\\
University of Michigan\\
{kaushikv,pmchen,jflinn,nsatish}@umich.edu
\end{large}
\end{center}

\begin{multicols}{2}
\renewcommand
\abstractname{摘要}
\begin{abstract}
\indent 数据竞赛\footnote{Data race}在多线程程序中是错误的一种普遍的来源。在这篇论文中，我们展示在运行中，如何通过执行多个带着补充线程调度\footnote{complementary schedule}的程序的副本来防止程序出现由数据竞赛而产生的错误。补充调度是一个线程副本的集合，他们保证副本只有在数据竞赛发生时才会偏离，而且基本上只有恶性的数据竞赛才会导致偏离。我们的系统，名为Frost，使用补充调度来使至少一个副本会避免会导致错误程序运行和最恶性的数据竞赛的指令顺序。Frost引入了基于收益的竞赛观测，它通过比较副本执行补充调度的状态来检测数据竞赛。我们会展示这种方法比当前的针对非托管代码的动态数据竞赛的检测法要快得多。为了帮助程序在生产中免除故障，Frost也会诊断数据竞赛的故障并采取适当的恢复策略，比如选取一个看上去是对的副本，或者执行更多的副本来获取更多的信息。\\

\indent Frost通过在一个单核上非抢占式地运行一个部分的所有线程在控制部分的线程调度。为了将程序规模扩展到多核，Frost 并行地运行第三个副本来产生程序将来可能的检查点，这些检查点使Frost能将程序执行分成几个时段，然后即可让它们并行执行。\\

\indent 我们用11个在桌面或者服务器应用环境下的真实的数据竞赛故障在测评Frost。Frost将这些故障全部检测并且度过了。因为Frost运行了3个副本，因为它的效用比是3x。不过，如果有空闲的核心来吸收这些额外的负荷的话，Frost只增加了$3-12\%$的开销。
\end{abstract}

\renewcommand
\abstractname{分类描述}
\begin{abstract}
\noindent D.4.5 [Operating S ystems]: Reliability; D.4.8[Operating Systems]: Performance
\end{abstract}

\renewcommand
\abstractname{着眼点}
\begin{abstract}
\noindent 设计，表现，可靠性
\end{abstract}


\renewcommand
\abstractname{关键词}
\begin{abstract}
\noindent 数据竞赛检测，数据竞赛生还，单并行
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%正文开始%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%


\section{引言}
多核处理器的流行鼓励了多线程程序在多个领域的应用，比如科学计算，网络服务器，桌面应用，和移动终端。不幸的是，多线程软件对于由数据竞赛，即两个线程中的两条指令（至少有一条是写）未经同步操作使用了同一个内存地址，而产生的故障十分脆弱。许多并行的故障都来源于数据竞赛。这些故障在生成中很难发现，并且可能会在运行中导致崩溃或者其他错误。由竞赛导致的错误可能是灾难性的，由2003年东北美的灯火熄灭和Therac-25的辐射超标即可见一斑[23,25]。\\

研究者们已经就在生产中消除或者检测数据竞赛提出了各种各样的方法，如纪律预言[5,7]，静态竞赛分析[13]，和动态竞赛分析[15,40,42]。尽管有这些努力，数据竞赛还是在折磨生产中的代码，并且是崩溃，错误，入侵的一个主要来源。\\

为了帮助解决数据竞赛故障的问题，我们提出运行一个程序的多个副本并且强迫两个部分执行\emph{补充调度}。我们使用补充调度的目的是强迫副本只有在出现潜在的恶性数据竞赛时才产生偏离。我们通过利用在可能的进程调度中的一个最佳点来做到这一点。首先，我们使所有的副本输入相同的输入并使用遵从由同步时间和系统调用所引发的同一个程序顺序限制的线程调度。这保证了不会执行一对竞赛指令的副本不会偏离[37]。接着，在遵从前一个限制的前提下，我们试着让2个副本执行的线程调度尽可能的不同。就是说，我们试着去最大化二条在不同线程没有被同步指令或者系统调用排序过的指令在两个副本中按不同顺序执行的可能性。对于我们研究过的所有的在真是应用中的恶性数据竞赛，这个策略导致了副本偏离。\\

我们的系统Frost，使用补充调度来打到两个目的：低成本地检测数据竞赛，和通过在运行时消除恶性数据竞赛的效果来增加程序的可行性。Frost引入了一个新的方法来检测竞赛：基于输出的数据竞赛检测。传统的方法通过分析程序引发的事件来检测数据竞赛，而基于利益的竞赛检测检测的是一个数据竞赛的效果，这点是通过比较不同的执行补充调度的副本的状态得到的。基于输出的竞赛检测获得了更低的能耗，但它可能会检测不到一些竞赛，比如，一些需要中断来引发一个错误的数据竞赛或者对多个无中断的调度都生成同样正确的输出的（完整地论述在3.4节）。不过，在我们对真实程序的测评中，Frost检测出了所有的潜在的那些被传统的检测器检测出的恶性数据竞争。虽然先前的工作[31]比较了多个指令排序的输出来分辨这些数据竞赛是无害的或者有害的，Frost是构造多个调度来检测和生还\emph{未知}数据竞赛的第一个系统。Frost因此面临必须针对未知的竞赛来构造调度。Frost从早先的分类工作中继承的一个优势是他自动地过滤了大部分被传统动态竞赛检测器认为无害的竞赛。\\

对生产系统，Frost不但能够检测，还能够检测和生还恶性的数据竞赛。因为一个由数据竞赛产生的并行故障，只会在一个特定的内存读取顺序下，执行补充调度能在很大程度上使得某一个副本躲过了数据竞赛的恶性作用。因此，一旦Frost发现了一个数据竞赛，他分析不同副本的输出，并采取一个最有可能掩盖掉错误的调度，比如分辨并继续执行正确的副本，或者增加副本来获得一个正确的副本。\\

为了生成补充线程调度，Frost必须严格控制每个副本的执行。为了做到这点，Frost把一个副本的线程时间切片到一个单核上，并只在同步时间点切换线程（使用无中断的调度）。在一个单核上运行无中断的各个线程还有一个好处：它防止了那些在优先权下的故障（如原子冲突），因此增加了可靠性。因为在单核上跑所有线程防止了某个副本扩展到多核，Frost使用单并行[44]，通过在多核上同时运行那个程序的多个时间片来并行化一个单核的执行。\\

Frost帮助解决多个环境下的数据竞赛问题。在测试中，它能充当一个快速动态数据竞赛检测器的角色，分辨出一个数据竞赛是无害的还是有潜在威胁的。对于一个测试或者处于生产中的有活跃用户的系统，检测和可靠性都是重要的目标。Frost遮盖了许多数据竞赛的错误，同时给开发者提供了可能会引发数据竞赛的程序的报告。\\

这篇论文做出了下列贡献。首先，它提出补充调度的概念，保证了副本在没有数据竞赛的时候不会偏离，并让副本在恶性数据竞赛很可能会偏离。第二，它展示了一种实际且低延迟的方法去使用补充调度去运行两个副本，这是通过用第三个副本来加速前两个副本的运行来实现的。第三，它展示了如何去分析三个副本的输出来打造一个生还数据竞赛的策略。第四，它介绍了一个新方法来检测数据竞赛，它拥有比传统动态数据竞赛检测更低的能耗。\\

我们在11个真实的桌面环境和服务器环境下的数据竞赛故障中测评补充线程调度的效率。在每一次测评中，Frost检测并且生还了所有这些故障。Frost的能耗在最坏情况下是3x，但如果有足够的核心或者空闲的CPU周期，它只有$3-12\%$的消耗。

\section{补充调度}

Frost的核心思想就是使用补充调度执行两个副本来检测和生还数据竞赛。一个数据竞赛由两条访问同一内存空间的指令组成（至少一条为写指令），以至于应用的同步限制能够让这两条指令以任意次序执行。对于恶性的数据竞赛，一个次序会导致程序出错（如果两个次序都导致出错，那么错误的根源不在于同步性的缺乏）。我们称那个会造成错误的顺序为一个\emph{故障要求}。一个数据竞赛的故障可能有多个故障要求，它们都必须触发才会引发程序崩溃。\\

作为一个例子，考虑图1(a)中的简单故障。如果线程1在线程2解析指针之前将fifo设置为NULL，程序就将崩溃。而如果线程2先解析了指针，程序就会正确执行。在图中的箭头指出了故障需要。图1(b)指出了一个复杂一些的原子冲突。这个故障有两个故障需要，就是说，两个数据竞赛就必须以某种顺序执行。\\

为了解释补充调度概念下隐含的准则，我们先考虑一个至多只有一个数据竞赛故障，并且没有任何会导致顺序限制的同步（我们把这种执行序列成为无同步）的执行序列。对于这样的序列，补充调度为数据竞赛的检测和生还提供了坚强的保证。我们在这节分析这些保证，不过，真实的程序不止有这些区域，因此在3.4节我们讨论如何推广这些能被Frost保证的区域。\\

以补充调度执行两个副本的目标是保证有一个副本躲过了数据竞赛的故障。当每对没有被系统同步排序的指令对a和b，在一个副本中a 在b前，在另一个副本中b在a前时，我们称两个副本有完美的补充调度。\\

\begin{center}
\includegraphics[scale=0.5]{1.png}

图1：数据竞赛的例子
\end{center}

为了达到完美补充调度，去除中断是必须的。 为了理解这点，考虑考虑一个有中断的调度：线程A执行了指令$a_0$，然后线程B 中断了线程A然后执行了指令$b_0$，然后线程A继续执行了指令$a_1$。不可能给这个调度生成一个完美的补充调度。在这个调度中，$a_1$先于$b_0$，然后$b_0$先于$a_0$。不过，这样就使得$a_1$先于$a_0$，违反了线程内的执行顺序。\\

反过来，没有中断时，给无同步的两个线程的一段指令序列构造一个完美的补充调度是简单的--一个调度只需把A的所有指令放在B之前执行；另一个调度把B的所有指令放在A之前执行。对于超过两个线程，一个调度按某个顺序执行那些线程，另一个调度按相反顺序执行那些线程。\\

因此，为了保证至少一个副本在无同步的时间段里避免了一个特别的数据竞赛故障，我们执行两个副本，并使用无中断的调度，并且让两个副本的调度顺序是相反的。\\

上面那个算法为一些普遍的数据竞赛故障提供了更好的性质。比如，考虑图1(b)中的原子冲突。因为故障需要指向两个不同的方向，两个调度会各满足一个限制但不会满足两个。既然故障需要两个要求都被满足，我们提出的算法保证了两个调度都躲过了这个故障。\\

一般说来，给了n个线程，我们必须任选一个顺序给一个调度，然后把那个顺序反过来给另一个调度。想象一下线程按这个顺序排好序，如果所有的故障需要指向一个方向，那么算法保证了有一个调度不会发生故障。在这篇论文的其他部分，我们会把这类故障称为“类型I”的故障。如果有两个故障需要指向不同方向，那么算法提供了更强的保证：两个副本都不会发生故障，我们把这类故障称为“类型II”。\\

\section{设计和实现}

Frost使用补充调度来检测和生还数据竞赛。这一节描述了几个挑战，包括估计有同步指令和一些故障的程序段的理想行为，使用多核执行评价表现，和使用启发式方法来检测正确的和错误的副本。我们将以Frost可能会无法检测和生还一些竞赛的情况和Frost如果去极小化这些情况带来的影响的讨论来结束本节。

\subsection{构造补充调度}

Frost把程序执行分成了很多时间片，称为\emph{小片}。在每个小片，它运行了多个副本，并且控制每一个的线程来达到一些性质。\\

Frost的第一个性质就是每个副本都遵循同一个系统调用和同步指令的顺序。换句话说，一些事件对，如对一个锁的lock和unlock，对一个条件变量的signal和wait，或者对一个管道的read和write，都表示了两个线程中的事件有先后关系；比如，lock之后的事件不会在unlock之前发生。通过确保所有线程对这种事件都有相同的先后顺序，Frost保证了两个副本只有在小片中出现了数据竞赛故障才会在输出或者内存状态中偏离[37]。另外，所有的副本都会遇见发生竞赛的指令对。\\

在第一个性质满足之后，Frost试图达到的第二个性质就是两个副本将拥有两个尽量互补的调度。根据我们在第2节论述的，这个性质是为了保证补充调度中至少一个副本不会因为某个数据竞赛而崩溃。\\

Frost必须执行一个副本来观察同步操作和系统调用的先后关系，之后才能在其他副本上加入相同的操作。为了观测那些顺序，Frost 使用了一个调整过的glibc，并在Linux的终端中给每个线程和同步个体（如锁和条件变量）维护了一个向量钟。向量中的每个值代表了一个线程的虚拟时间。同步时间和系统调用会在本地向量钟中增加相应线程的值。类似unlock的操作会把那个锁的在向量钟中的值设为之前它的值和unlock的线程的向量钟值的较大者。类似lock的操作会把线程的值设为它之前的值和锁的值的较大者。类似的，我们调整了中断的个体使它们包含向量钟并且在相关的系统调用中传播这些讯息。例如，类似map和munmap的系统调用不能交换，Frost 给进程的地址空间加了个向量钟，来获得一个调整地址空间的系统调用的全序。\\

当第一个副本执行时，Frost把所有系统调用和同步记录到了一个日志了。其他的副本读取这些记录好的值并使用它们来遵循相同的发生先后关系。每个副本维护一个\emph{回放向量钟}，它记录线程调用同步指令或者系统调用的情况。在回放向量钟相等或者超过第一个副本记录的值之前，线程不能继续执行。这保证了，举个例子，直到另一个线程unlock，一个线程不会从lock返回（如果这两个操作在原副本中有先后关系）。同一时间可以同时执行超过一个副本的小片；不过，其他的副本会稍微落后第一个副本，因为他们不能执行一个同步指令或者系统调用，在第一个副本执行完之前。\\

给定了一个先后发生顺序，Frost使用下列下列算法来来给两个副本构造调度使他们尽量得互补，但又没有修改原本的程序。Frost在一个副本中给所有线程选择了一个排序，然后给第二副本一个相反的排序。比如，三个线程在一个副本排序为[A,B,C]，那么在另一个副本中就排序为[C,B,A]。Frost在单核上执行每个副本的所有线程因此两个线程不会同时运行。只要一个线程没有在等待去满足一个先后发生关系并且没有跑完当前的小片，它就能继续执行。Frost的终端永远都执行调度中的第一个且能继续执行的线程。一个线程会持续执行知道当前小片结束，它会因先后关系，或者副本线程调度而阻塞。\\

\subsection{使用单并行来扩展}

就像我们到现在所描述的那样，补充调度不允许程序通过多核来扩展因为一个副本的所有线程都必须在一个2单核上顺序执行。如果一个副本的多个线程要同时在不同的核心上执行两条指令，这两条指令不会被一个先后发生限制所排序，因此可能会产生竞争。在这时候，这两个副本应该以不同的顺序执行这两条指令。不过，确定顺序，并让另一个副本以相反顺序执行就表明了这两条指令要顺序执行，而不是并行。\\

Frost使用单并行[44]来取得扩展性。单并行是建立在一个结论上的：至少两个方法来让一个多线程程序在多核上运行。第一个方法，称为\emph{线程并行}，在不同的核心上跑多线程，是利用并行的一个传统方法。第二个方法，称为\emph{小片并行}，并行地运行多个时间片。\\

单并行执行一个程序的线程级并行或者多个小片级并行。它限制每个小片级并行的执行来使他们在一个单核上执行。这个策略允许了小片级的并行来获得单核上运行的优势。我们对单并行原本的使用是在一个叫DoublePlay的系统，它能高效的软件决定性回放[44]。Frost是建立在DoublePlay的基础上修改来的，不过他出于一个不同的目的使用单并行，即使用以相同先后关系的补充调度来执行副本。\\

\begin{center}
\includegraphics[scale=0.5]{2.png}

图2：Frost的概览
\end{center}

就像在图2中展示的那样，为了并行地执行小片，一个单并行的执行会生成每个小片的起始点。他必须尽早产生这些起始点来在之前的小片执行完之前开始执行之后的小片。因此，线程级的并行在小片级并行之前跑，并且产生未来小片的起始点。多个小片并行地执行，类似处理器的流水线--这使得小片级并行能随着能用的核心数而扩展。\\

总之，Frost给每个小片跑三个副本： 一个线程级并行地副本用来记录每个小片的发生先后关系，并且为另外两个副本的并行运行生成检查点,还有两个小片级并行的副本以补充调度运行。副本使用写入时复制的分享来减少总共的内存开销。Frost使用在线的决定性回放[24]来保证所有副本获得了同一个非决定性的输入来使得所有副本都有相同的发生先后顺序。他在线程级并行的副本执行时，记录了所有系统调用的结果和同步指令。当小片级并行的副本在之后执行相同的操作时，Frost的终端和glibc库不会重新执行操作，而是直接返回已经记录好的值。信号也在线程级并行的副本中被记录好，并且在小片级并行的同一点上发出。因为Frost记录并且回放了所有除了数据竞赛之外的非决定性因素，只有数据竞赛会使得副本发生偏离。在线回放还有一个好处-- 小片级并行不会因为I/O而阻塞，因为线程级并行已经获得了结果。\\

当副本在一个小片内偏离时，Frost会在一些行动中选择一个。首先，它可能会接受某一个副本执行的结果，我们把这个叫做\emph{递交}那个副本。如果它选择递交线程级并行的副本，它就是简单的放弃了小片开始的检查点。如果它选择递交了一个小片级并行的副本，并且那个副本的内存和寄存器状态和线程级并行的副本不同，那么流水线上之后的小片都是不正确的。因此，下一个小片开始的检查点是未来程序状态的不正确的一个提示。Frost先放弃递交的副本的开始的检查点。之后，它放弃所有递交的副本之后的所有小片。并且以递交的副本的状态来重新开始执行线程级并行和小片级并行的副本。\\

Frost也可能选择去执行更多的副本来获得更多关于导致偏离的小片的情况。他从那个小片的开端开始了一个额外的线程/小片级并行的执行，我们称这种这个过程叫做\emph{回滚}那个小片。Frost之后可以决定提交原来的副本或者新的一个，不过现在只有新的会被递交。\\

因为副本会给出不同的输出，Frost不会放出任何输出知道它决定了提交哪个副本。他使用了推测性的执行（使用Speculator[32]来完成）来推测输出。当我们决定一个小片应该执行多长时间时，引出了一个正确性，功耗，和输出延迟的权衡。更长的小片会对正确性有好处，就像我们在3.4.3节讨论的那样，和比较小的功耗。更短的小片会有更短的输出延迟。通过使用适应性的小片长度，Frost 平衡了这些限制。对于有关CPU的无外部输出的程序，小片长度会长达1秒。不过，当程序会执行产生外部输出的系统调用，Frost会马上开始一个新的小片。因此，我们测评的服务器程序会有一秒几百个小片。另外，就像我们在3.3.1节论述的，小片长度会随便观测到的数据竞赛的数量而变化--没有数据竞赛的小片会逐渐的增加长度（一次50ms)，然而出现数据竞赛的小片会减少小片长度（以最大20的比率）。当Frost决定开始一个小片之后，它会登台1所有线程去到达一个系统调用或者同步操作。然后它给程序设立检查点，并允许线程继续执行。

\subsection{分析小片的输出}

等到所有三个副本都完成了一个小片之后，Frost终端比较了他们的执行来找到并且生还数据竞赛。因为Frost的控制代码和数据在终端里面，下面的逻辑不会被程序级的故障所损坏。\\

首先，Frost测定一个副本是否已经崩溃或者进入了一个死循环。我们称这为\emph{自证明}的故障，因为Frost能宣称这样的一个副本已经故障了，而不用去考虑别的副本的结果。Frost使用干预终端的信号处理机制来检测一个副本是否已经崩溃或者退出。它使用一个基于超时的机制来测定一个副本是否已经进入了一个死循环（我们发现无需进行更加复杂的检测）。\\

其他类型的故障不是自证明的；比如，一个副本可能会产生不正确的输出或者中间的状态。一个检测这种故障的方法需要一个细致的对正确输出的描述。不过，对于网络服务器或者数据库这种复杂的程序，制作这种描述是非常骇人的。在实际中面对解决这个挑战需要一个不依靠程序语义或者手工制造的描述的检测不正确输出的方法。\\

Frost通过比较三个副本的输出和程序状态来猜测潜在故障的存在。在执行中，Frost比较了每个副本产生系统调用的顺序和参数。Frost还比较了每个副本在每个小片执行完后的内存和寄存器的状态。为了减少比较内存状态对Frost表现的影响，Frost只会比较被污染的页或者新分配的页。如果他们在小片中的状态或者的输出之一不同，Frost就认为两个副本有不同的输出。\\

为了检测并且生还数据竞赛，Frost必须猜测两个数据竞赛发生了和哪个副本故障了。Frost首先考虑哪个副本经历了自证明的故障。如果这个副本没有经历一个自证明的故障，Frost考虑这个副本在一个小片后的内存和寄存器状态，和在小片中副本产生的输出。\\

3个副本的结果有11种组合，在表1的左边一列中展示了出来。F指副本经历了一个自证明的故障；A-C指某个副本产生的状态和输出。我们给相同的状态和输出使用了相同的字母。为了简化说明，我们在这个展示里不区别各种故障的类型。第一个字母展示了线程级并行的结果，另外两个展示了小片级并行的结果。比如，F-AA表示线程级并行的副本有一个自证明的故障，但两个小片级执行没有经历一个自证明的故障并且产生了同样的状态和输出。\\

另外，两个副本可能会产生同样的输出和最终状态，但因为一个数据竞赛在小片中采取了不同的执行过程。因为Frost的补充调度算法，很可能这个数据竞赛的无害的，就是说两个副本都是正确的。因此，在小片中允许小规模的偏离是一个有用的优化。当会产生一个合理的结果时，Frost 让一个小片级并行的副本执行执行一个不同的系统调用（如果这个调用没有副作用）或者一个不同的同步指令。比如，它允许一个小片级并行的副本去执行没有被线程级并行副本调用的一个nanosleep或者getpid系统调用。它同样允许自取消的指令对，如一个unlock跟着一个lock。当更多的优化可能时，被这样的优化过滤掉的无害的竞赛的数量是比较少的。因此，增加更多的优化可能是不值得。因此，让一个偏离不能被上述的优化处理时，Frost就称两个副本有不同的输出。

\begin{center}
\includegraphics[scale=0.7]{3.png}

表一：epoch输出的分类
\end{center}


\subsubsection{使用小片的输出来生还数据竞赛}

Frost使用Occam的剃须刀来诊断结果：它选择能产生观察到的结果的最简单的解释。特别的，Frost选择需要一个小片内出现最少数据竞赛故障的解释。在相同数量故障的解释中，表1的中间那列展示了Frost和结果联系起来的解释，右边那列展示了Frost 根据那个解释采取的行动。\\

最简单的可能解释就是那个小片没有数据竞赛故障。因为所有副本遵从同一个发生先后关系的限制，一个没有数据竞赛的小片必然在所有副本中产生相同的结果，所以这个解释只能应用于A-AA和F-FF。对于A-AA小片，Frost认为小片在所有副本中都正确执行并且进行递交。对于F-FF小片，Frost断言小片因为一个非竞赛的故障在三个副本中都失败了。在这个情况下，Frost回滚并且从小片开端重新执行程序，希望这个错误是非决定性的，可能在一个不同的执行中被躲过，比如，这个故障是因为一个不同的发生先后关系而发生的。\\

第二个最简单的解释就是小片经历一个I类型的数据竞赛故障。单个I类型的故障会产生最后两个不同的输出（一个顺序一个），因此两个小片级并行的执行会不同，因为他们以不同顺序执行发生竞赛的指令。对于I类型的故障，有一个顺序不会满足故障需要，因此会正确执行。另外一个顺序会满足故障需要并且导致一个自证明的故障，不正确的状态，或者不正确的输出。下列组合的小片级并行副本有两个不同的输出（有一个是对的），并且最多只有两个不同：A-AB（A-BA)，A-AF（A-FA），和F-AF（F-FA）。\\

对于结果是A-AF和F-AF的结果，一个没有发生自证明的错误的副本比较可能是正确的，因此Frost提交那个副本。对于产生A-AB 的副本，我们不清楚那个是正确的（可能因为一个无害的竞赛，两个都是对的），因此Frost要收集更多的信息，它开启了新一组的三个副本，从小片的开端开始运行。在这时候，Frost首先试图去寻找一个能防止竞赛发生的发生先后限制；我们假设对于一个经过良好测试的程序，这样的执行很可能是正确的。对于我们已经测试过的数据竞赛，Frost经常在一个或者两个回滚后遇到这样的一个限制。这是因为结果的不同组合（比如，A-AA，Frost可以递交小片并且继续）。如果Frost在每个执行都碰见了数据竞赛，我们就认为最自然的执行可能是正确的，就是说，让Frost选择线程级并行的执行（这是最正常的执行）。注意，因为小片级并行的执行使用了手工的调度，他们不应该被看重。因为，我们不会认为A-BA给A投了两票，B一票，而认为只给A投了一票。\\

如果结果的一个组合不能被一个类型I的故障解释，下一个简单的解释就是一个类型II故障。一个类型II的可以产生下述结果组合：A-BB，A-BC，F-AA，和F-AB。这里没有一个应该是被一个类型I的故障引发的，因为小片级并行的副本产生了相同的结果（A-BB或者F-AA）或者因为有三种输出（A-BC或者F-AB）。在之后的那个情况下，输出不可能是由一个类型I的故障引发的，不过在第一个情况下，那个结果是不太可能的。任何小片级并行的执行都应该躲避类型II的故障，因为它无中断的执行不符合一种故障的故障需要。比如，原子冲突故障是一种线程交叉时引发的故障。因为线程不会在小片级并行中被中断，两个副本都会避开这种故障。\\

我们发现类型II的故障很普遍地会有三个不同的输出（比如，A-BC或者F-AB）。比如，考虑两个线程以一个没有同步形式在记录输出。线程级并行的副本不正确地将输出混合起来（输出A），一个小片级并行的正确的在第二个值之前完整地输出了第一个值（输出B），剩下的小片级并行副本也正确地完整地在第一个值之前输出第二个值（输出C）。在插入或者删除非同步的共享数据结构也会遇到相似的情况。因此当Frost遇见一个A-BC输出，它会递交一个小片级并行的副本。\\

剩下来的组合（A-BF，A-FB，或者AFF）不能被单个数据竞赛故障所解释。A-BF有多于两个输出，这样就不是一个单个类型I的故障了。A-BF也包含了一个失败的小片级并行的运行，这样就不是单个的类型II的故障了。两个小片级并行的副本都在A-FF失败，这也不可能是单个的类型I或者类型II的故障。我们的结论是这些组合是由一个小片内的多个数据竞赛故障引发的。Frost会回滚到小片开始检查点，然后执行一个比较短的小片长度（希望只遇到一个故障）。\\

\subsubsection{使用小片输出来检测竞赛}

使用小片的输出来检测数据竞赛比使用这些输出来生还数据竞赛更加容易。任何在系统调用，同步指令，或者最终状态中出现偏离的输出都表明在小片中出现了一个数据竞赛。因为这三个副本遵从相同的发生先后关系， 数据竞赛是唯一的副本偏离的理由。另外，所有的副本也是从同样的初始状态开始的。\\

因为Frost的数据竞赛检测是基于输出的，不是所有在小片中发生数据竞赛都会被报告。这是一个过滤无害竞赛的一个好方法，有时程序员会有意地加入来提升表现。特别的，一个ad-hoc的同步可能永远不会引发一个内存或者输出的偏离，或者，一个竞赛可能会引发一个暂时的偏离，比如马上就被覆盖的栈中的值。如果Frost尝试了一对指令的不同顺序并且没有报告一个竞赛，那么这个竞赛几乎就是无害的，至少在程序的这次执行中。唯一的例外，在3.4.4节中会讨论，就是多个故障导致了相同但不正确的程序状态或者输出。\\

既然Frost允许副本在小片中出现些许的偏离，它有时观察到副本在系统调用或者同步指令上有不同，但没有在输出或者最终状态没有出现不同。这种竞赛同样是无害的。Frost会报告这种竞赛的存在，但增加一个注解：没有在程序上产生明显效果。一个开发者能自己选择是否去解决这些竞赛。\\

因为Frost实在DoublePlay框架上做决定性的记录和回放的，它继承了DoublePlay的重新执行的能力[44]。因此，在报告了一个竞赛的存在之后，Frost也能根据需要重新把导致竞赛的程序给运行一遍，这样就允许一个开发者去选择他最喜欢的去故障工具。比如，我们在DJIT+[34]上面搭建了一个传统的动态数据竞赛检测器，它能回放一个偏离的小片来精准地确认出现竞赛的指令。

\subsubsection{采样}
一些检测竞赛的工具使用采样来以错过数据竞赛的代价来减少功耗[6,14,29]。我们给Frost增加了类似的选项。当用户确定了一个目标采样频率后，Frost就只给一些小片创造小片级并行的副本；我们称这些小片为\emph{采样过的小片}。Frost不会给别的小片执行小片级并行的副本，意味着它在这些小片中既不检测也不生还数据竞赛。Frost动态的选择哪些小片来采样，使得总的执行时间和采样频率相符。虽然可以使用更加复杂的机制来选择采样的小片，这个策略能使得Frost对数据竞赛检测和生还的能力减少和采样频率成正比。

\subsection{不足}

第2节讨论了补充调度能在没有同步且数据竞赛不多于一个的程序段中检测并生还数据竞赛的保证。我们现在讨论当小片不符合这些性质时的不足之处。我们也会讨论Frost是如何缓和这些不足之处的。就像在4.1.2节展示的那样，这些不足没有在实际测试中给Frost造成多大麻烦。

\subsubsection{一个小片中的多个故障}

尽管我们假定数据竞赛的故障比较少见，一个小片还是可以包含多余一个故障。如果在小片中出现了多个故障，Frost对这个执行的诊断可能是不正确的。这会影响生还和检测数据竞赛。\\

生还数据竞赛需要至少一个副本正确地执行。给一个小片增加任意多的类型II的故障不会影响生还，因为两个小片级并行的副本都不会因为这种故障而失败。因此，当一个小片有零个或一个类型I的故障再加上任意多个类型II的故障时，都会有一个副本是正确的。但是，多个类型I的故障的存在会导致两个副本都失败。一般来说，不同的故障会导致程序以各种方式来崩溃。失败的表现（如崩溃或者退出）可能是不同的，或者内存和寄存器状态在失败的时候也会是不同的。因此，Frost仍然可以采取更正措施，如回滚，或者执行额外的副本，特别当失败是自证明的时候。当Frost回滚时，它会减少小片长度来在重新执行时将故障分隔开来。这像是一种搜索。\\

有可能（虽然很少见）两个不同的类型I的故障在程序状态上有相同的作用，这时补充调度的副本会到达相同的最终状态。如果失败不是自证明的，Frost会错误地将小片归类，并且递交了一个错误的状态。\\

为了检测数据竞赛的目的，多个数据竞赛只在所有竞赛对程序状态有同一个作用时会成为一个问题。不然，副本会偏离，然后Frost 会给这个小片报告一个竞赛。当回放时，开发者就会发现多个数据竞赛。

\subsubsection{优先级反转}

先后关系限制可能会导致Frost没有生还或者检测到一个小片内的数据竞赛。对于那种只有线程互相交互的小片，Frost的补充调度算法会构造出使所有潜在的会竞赛的指令顺序不同的调度。不会产生竞赛的指令可能会以相同顺序执行，不过，根据定义，这不会影响到数据竞赛的保证。\\

当多于两个线程在小片中交互时，一个类似于优先级反转的情况会发生，并且防止Frost改变所有不发生竞赛的指令的顺序。比如，考虑图三。这个小片包含了三个线程，一个由于同步的发生先后关系限制，和一个由两个竞赛的指令引发的故障需要。如果Frost 将ABC 的顺序设定给了一个副本，两个副本中的执行顺序就是$\{a_0,b,c_0,a_1,c_1\}$，和$\{c_0,c_1,b,a_0,a_1\}$。 所有的代码段对在两个调度中都以不同顺序执行，除了两个例外。在两个副本里，$c_0$都在$a_1$之前执行。不过，这个顺序是程序的同步所需的，并且那个同步防止了这些指令出现竞赛。另外，$b$在两个调度里都在$a_1$之前执行。如果图中的类型I的故障出现的话，那么两个副本都会失败。如果失败不是自证明的且没有在线程级并行中出现的话，这可能会防止Frost生还或者检测竞赛。 \\

\begin{center}
\includegraphics[scale=0.7]{4.png}

图三：优先级反转的情况
\end{center}

注意Frost可以给三个线程选择另一组性质，比如BAC。基于这个观察，我们设立了一个机制，能够帮助选择线程的特性来防止优先级反转。就在程序执行时，Frost记录每对线程的发生先后限制的数量。它使用一个贪心算法来将两个发生先后限制最多的两个线程安排了相邻的优先级，然后再放入同样限制最多的线程，这么下去。因为优先级反转只会在在优先级中2个非相邻的线程上，只要之前的限制是对之后限制的一个比较好的预计，这种机制就能减少优先级反转的几率。\\

在某些情况下，一个小片的线程级并行执行可能在小片级并行执行之前完成。在这种情况下，Frost可以发现那个小片特有的发生先后限制，并且以此来选择线程的性质。但我们还没有使用这个优化。

\subsubsection{小片的边界}

Frost把执行分成小片来使用多核获得扩展性。每个小片代表了一个顺序限制（一个障碍），这个障碍在原本的程序中是不存在的。如果一个错误需要经过了一个小片边界（一条指令在一个小片中，另外一个在下一个小片里），这两条指令的顺序在所有副本中都是固定的。对于类型I的故障，所有副本都会失败或者都会躲过故障。\\

为了减缓这个不足，Frost采取了两步。首先，它以比较小的频率构造小片。然后，它当所有线程都在执行一个系统调用时创建了一个小片。对于一个类似于原子冲突的数据竞赛，这保证了没有副本会失败，除非程序在一个原子区域执行了一个系统调用。\\

所有的用来生还恶性竞赛的系统（如Frost），都必须在输出之前提交状态，并且继续执行是以输出为前提的。为了避免一个恶性竞赛引发的故障，这种系统必须回滚到某个在故障前提交的状态。 这个提交的状态可能人工地将提交点上下将指令定序，并且这个顺序限制可能会强迫程序经历一个恶性数据竞赛[26]。\\

当Frost只被用来检测而不生还数据竞赛时（比如在测试时），可能就没有必要在数据竞赛发生后保持外部输出的稳定了。因此，我们使用了一个优化：当Frost只被用来检测时，外部输出不会引发下一个小片的创立。这个优化只在4.2节被使用。


\subsubsection{类型II故障的检测}

Frost基于输出的竞赛检测可能不能检测到某些类型II的故障。检测需要任意两个副本在系统调用或者同步指令中不同或者两个副本在小片末尾有不同的内存或者寄存器状态。就像之前提到的，一些无害的竞赛有这个性质--过滤掉这种竞赛基于输出的竞赛检测的一个优势。另外，如果两个竞赛的指令集合相同，一个代码段也会有这种性质。当两个小片级并行的副本都正确并且以同一状态完成小片时，这种情况就很容易发生。不过，在我们到目前的经验来看，类型II的故障总是会在程序状态或者输出中有一些不同。\\


\section{检测}

我们的检测回答了以下几个问题：\\
Frost能多么有效地生还数据竞赛的故障？\\
Frost能多么有效地检测这样的故障？\\
Frost的功耗怎么样？\\

\subsection{生还和检测竞赛}
\subsubsection{方法论}
我们用一个4GB的DRAM内存，由两个2.55GHz四核Xeon处理器组成的8核服务器来测评Frost检测和生还数据竞赛的能力。操作系统是CentOS Linux 5.3，Linux 2.6.26终端和2.5.1的GNU库，都被调整过来支持Frost。\\

我们使用11个真实的并行故障来测评。由重现一个在线的并行故障集合[50]开始（Apache,MySQL和pbzip2)，他们是从几个学术资源那里编译过来的[27,49,51]，还有BugZilla的数据库。从12个并行故障中，我们重现了所有9个数据竞赛故障。另外，我们重现了应用pfscan中的一个数据竞赛故障，它曾经在学术讲座上被使用过[51]。最后，在我们的测试中，Frost检测到了glibc中一个之前不知道，可能是恶性的数据竞赛。表2列出了故障并且描述了他们的作用。\\

对每个故障，我们做了5次测试，在测试中故障会在Frost的监护下起作用。表2的第4列展示副本对包含故障的小片的输出。第5 列展示了Frost以一个没有故障的输出来生还数据竞赛的百分比。下一列展示了Frost根据输出或者状态检测到故障的百分比。 最后一列展示Frost要花多久才能从故障中恢复过来，这包括了回滚和执行新副本的代价。

\end{multicols}

\begin{center}
\includegraphics[scale=0.7]{5.png}

表2: 数据竞赛的检测和生还
\end{center}


\begin{multicols}{2}

\subsubsection{结果}

这些实验的主要结果就是Frost检测并生还所有11个故障的5次检测。对于这些应用，生还数据竞赛增加了一些执行功耗，主要是因为小片对于如MySQL何Apache的应用来说太短了，并且另外应用中的故障在执行的末尾发生，因此撤销之后的工作几乎没有减少功耗。我们接下来给每个故障提供更多的细节。\\

pbzip2数据竞赛在一个工作线程对主线程解法的一个指针解引用时可能触发一个SIGSEGV。这是一个类型II的故障因为解引用必须在重新分配之后而且在主线程结束之前。这种失败是自证明的，导致了一个F-AA的小片输出。\\

Apache的

\subsubsection{我们的基础存储系统cache}

\indent 我们的基础cache是一个传统的, 写回的cache, 用LRU的替换策略. 最近研究表明固态的LRU-cache对于企业级的工作负荷来讲并不是很划算的. 在我们的评估中, 我们也确认了这个结果, 并且我们改进了它从而证明一个传统的LRU算法在应用了差异化存储服务之后也可以变得划算. LRU算法更好的算法甚至可以提供更好的结果.

\indent 一个固态储存盘被用作cache, 然后我们将他分割成可配置个数的分配单元. 我们用八个扇(4KB, 一个常用的内存分页大小)作为分配单元, 然后我们将所有这些分配单元连续添加到空闲队列\footnote{free list}里以初始化cache. 最初, 空闲队列包含固态存储的每一个部分.

\indent 对于新的写请求, 我们用这个空闲队列中分配缓存项. 一旦被分配, 这些项就被移除空闲队列, 然后被加到一个已用队列\footnote{dirty list}中. 我们通过记录一个逻辑块号作为关键字的哈希表中的对应, 记录分配给每个I/O的项.

\indent 一个syncer守护进程\footnote{syncer deamon}管理着空闲队列的大小. 当空闲队列的大小小于一个很低的标记\footnote{watermark}的时候, syncer开始清理已用队列. 已用队列按照LRU的顺序排序. 当已用的单元被读或写的时候, 他们被移动到队列的最后. 这样, syncer就能首先清理最近最少使用的单元. 当单元被清理的时候, 他们被重新添加到空闲队列中. 空闲队列同样按照LRU的顺序排序, 所以如果空闲队列中的项被访问的话, 它将被移动到队列的最后.

\indent 这些都在那个我们引入了选择分配和选择收回的基础cache的基础上的.

\subsubsection{传统分配方式}

\indent 当现在的操作系统要决定是否为一个I/O分配cache的时候, 有两种探索法经常被使用. 这与请求的大小还有访问方式(随机或连续)有关. 例如,NTFS文件系统中一个256KB的请求说明I/O请求的文件大小至少有256KB, 多个连续的256KB的请求则说明那文件可能会更大. 小的随机的请求从cache中受益最大, 所以大的请求或者可能是一个连续文件流的一部分的请求就经常被cache忽略, 因为这样的请求只被磁盘处理的话也效率相当. 可是这种方法有两个基础性的问题.

\indent 首先, 块层的请求大小可能只有一部分与文件大小有关. 小文件也可以被很大的请求使用, 大文件也可以被小的请求使用. 这只是由应用的请求大小还有cache模型(例如: 缓存的还是直接的)决定的. 一个典型的例子是NTFS的主文件表(MFS). 这个关键的元数据结构是一个巨大的, 经常被顺序写入的文件. 尽管当读取的时候, 请求经常是小且随机的. 如果一个文件系统想要在MFS被写入的时候想忽略cache, 随后的读操作也会进入磁盘. 要想解决这个问题, 需要主文件表从其他文件中区分出来, 但是没有I/O分类机制的话这可不简单.

\indent 第二个问题就是操作系统有一个请求大小的上限(例如: 512KB). 如果一个人要基于请求的大小做出是否进入cache的决定, 他无法分辨比这个请求大小上限更大的文件大小. 这对小的DRAM cache不是问题, 但是固态储存的cache都是很大而且可以装很多文件的. 所以, 知道一个文件是1MB还是1GB是对于做cache决策很重要的. 例如, 它如果能缓存更多的小文件比缓存很少的大文件更好, 这就正是那些不允许查找小文件和他们的元数据的文件服务器中的情况.

\subsubsection{选择分配}

\indent 因为以上的问题, 我们不能根据请求的大小做cache分配的决定. 反倒, 在这个文件系统原型中, 我们分辨出元数据和数据, 然后我们再依据文件大小分辨文件.

\indent 元数据和小文件总要进入cache, 大文件要依据情况是否进入cache. 我们现在的实现就是检查syncer守护进程是否是活动的(清理已用队列), 活动的表示cache被大量占用, 那我们的优化就不要缓存大文件(我们的切断点是1MB或者更大, 这样的数据块就会跳过cache). 另一方面, 一个空闲的syncer守护进程就说明cache里还有一定多的空间, 所以我们甚至可能把最大的文件也放入cache 中.

\subsubsection{选择收回}

\indent 选择收回和选择分配在其利用优先级信息的时候是相似的. 我们收回优先级最低的单元, 而不是按照LRU的顺序收回单元. 这通过对每一个I/O分类维护一个已用队列实现.当cache的空闲单元个数达到一个下界的时候, syncer就清理已用队列中优先级最低的一个. 当这个队列空了, 它就选择下一个优先级最低的队列, 然后以此类推. 直到达到一个空闲单元的上界标记, 然后syncer就进入休息状态.

\indent 用选择收回的方法, 我们就可以填充cache的同时避免交错优先级\footnote{inversion}, 对于一个文件系统, 这允许了缓存更大的文件. 却不用付出收回小文件的代价. 大文件当cache使用紧张的时候就会被收回,剩下小文件和元数据有效地留在cache 中. 优先级高的I/O只有在更低优先级的数据全部被收回之后才会被收回. 当我们演示我们的性能评估的时候, 在这个企业级工作负荷中, 其中混合了大量很现实的大小不一的文件, 小文件和元数据很少会被收回.

\subsubsection{Linux实现}

\indent 我们引入一个SW cache作为9级RAID\footnote{磁盘阵列}, 在Linux磁盘阵列栈之上(MD)\footnote{作者: RAID-9并不是一个标准的 RAID 级, 但是是一个为Linux MD引入cache空间的一个简单方法.}. 这对RAID的映射是自然的.RAID级(例如: 0, 1, 5) 和嵌套的版本(例如: 10, 50)简单地定义了一个从逻辑块空间到存储设备上物理块的一个静态的映射. 例如RAID-0, 就定义了逻辑块应该被循环地分配. 一个差异化存储服务结构, 与之对比, 就提供了一个动态的对应.在我们的实现中, 分类策略和对应的分配的策略提供了一个向存储设备或者cache的对应, 虽然可能还要考虑向多级cache或者多个存储池的对应.

\indent 将cache作为RAID设备管理允许了我们在已有的RAID管理工具之上进行构造. 我们用Linux mdadm工具去创造一个cache 空间. 可以简单地定义存储设备和cache设备(/dev中的设备), 二者都可能是另一个RAID空间. 例如, cache设备可能是一对镜像固态储存盘, 存储设备是个RAID-50阵列. 这样实现差异化存储服务使得可以简单地在已有的存储管理工具之上做集成.

\indent 我们的SW cache是在一个在cache空间被创造时加载的内核RAID模型中实现的; 和分类策略还有优先级分配的信息被作为运行环境变量被传递到模型中. 因为这个模型是内核的一部分, I/O请求在块层被中止, 并无法接触SCSI层. I/O分类信息于是被直接从块层I/O请求(BIOs)中提取出来, 不是SCSI请求中5比特的分类信息字段.

\subsubsection{iSCSI实现}

\indent 我们的第二个存储系统原型是基于iSCSi的. 与RAID-9原型不同, iSCSI是与操作系统无关的, 可以既被Linux也被Windows使用. 在两种情况中, I/O分类信息被拷贝到主机的SCSI请求中. 在iSCSI目标上, I/O分类信息被从请求中提取出来, I/O类的优先级被确定, cache策略也被确定. cache的实现和RAID-9原型是一样的.

\section{评估}
\indent 我们用一个文件服务器工作负荷来评估我们的文件系统原型(基于SPECsfs), 一个电子邮件服务器工作负荷(模型来自瑞士网络分析), 一系列文件系统工具(find, tar和fsck), 还有一个数据库的工作负荷.

\indent 我们提供的数据源自Linux RAID-9为文件系统工作负荷的实现； NTFS的结果也类似地运用了我们的iSCSI原型. 对于Linux TPC-H, 我们也运用了iSCSI.

\subsection{试验的配置}

\indent 所有的试验都在一个Linux机器上运行. 我们的Linux系统是一个双通道四核Xeon服务器系统(8核心), 有8GB的内存. 我们运行2.6.34内核的Fedora13系统, 在第4部分中介绍过我们进行的更改. 这样, Ext3文件系统就被更改得可以分类所有的I/O, 块层将分类信息拷贝到Linux BIO中, 然后BIO被我们的cache原型使用(一个Linux RAID(MD)栈中运行的内核模型).

\indent 我们的存储设备是一个有5块磁盘的 LSI RAID-1E阵列. 在这个基层设备之上, 我们配置了一个我们在4.4.5或4.4.6(为TPC-H)部分中描述的cache; 一块英特尔32GB X25-E SSD被用作cache. 对于我们的每一次试验, 我们只用我们硬盘可用空间的一部分(10-30\%)配置一块cache.

\subsection{工作负荷}

我们的文件服务器工作负荷基于SPECsfs2008; 表8 (文件系统)中展示了文件大小的分布. 安装阶段在8738个目录中创造了262144个文件(SFS规定了每个目录30个文件). 标准测试程序进行了262144次处理, 其中一次处理是读取一个存在的文件或者创建一个新文件. 读写比是2:1. 这次测试运用的总空间是184GB, 我们配置了一个18GB大的cache(文件池大小的10\%). 我们在文件处理的最后维持文件池, 然后运行一系列文件系统工具. 具体地讲, 要查找一个不存在的文件(find), 将文件系统存档(tar), 然后检查文件系统的错误(fsck).
\begin{center}
\includegraphics[scale=0.5]{9.png}

表8: 文件大小分布.
\end{center}

\indent 我们的电子邮件服务器工作负荷是基于对电子邮件服务器文件大小的研究的. 我们使用4KB的请求大小, 还有2:1的读写比. 安装过程创建了1000个目录中一百万个文件, 然后我们在文件池中做一百万次处理(读取或者创建邮件). 表8(电子邮件系统系统) 中展示了文件大小的分布. 这个测试使用了204GB的磁盘空间, 我们另配置了一个20GB的cache.

\indent 最后我们在修改过的PostgreSQL数据库上运行TPC-H决策支持工作负荷. 每一个PostSQL文件打开的时候都带有0\_CLASSIFIED 标识, 也就是说允许了用户层的分类, 并且禁止了Ext3中的文件大小分类. 我们用8作为规模参数构建了数据库, 在磁盘上占用了29GB 的空间, 然后我们连续地运行了I/O为主导的询问操作(2, 17, 18, 和19). 我们对比8GB的LRU和LRU-S cache.

\subsection{测试方法}

\indent 我们用一个内部的, 基于文件的工作负荷生成器, 为文件和电子邮件服务器工作负荷生成数据. 作为输入, 生成器要接收一个文件大小的分布, 一个请求大小的分布, 一个读写比, 和子目录的个数.

\indent 对每一个工作负荷, 我们的生成器生成特定个数的子目录, 而且在这些子目录中生成满足文件大小分布和写入请求大小分布的文件. 当文件池被创造之后, 就要对它做一系列操作, 用生成的这些文件还有之前的请求大小分布. 我们记录每秒钟读写的文件个数, 还有对于每个文件, 延迟的95\%概率大值(最差情况), 或者读/写整个文件所需时间.

\indent 我们对比三种配置的存储设备的性能. 没有固态储存的cache, 一个LRU-cache, 还有一个用选择分配和选择收回的升级版cache(LRU-S). 对于有cache的测试, 我们同样对每一个类记录cache空间的组成, 读取的命中率, 还有移除开销\footnote{eviction overhead}(相比于清理cache, 转移数据块的比例). 这三种材料可以说明LRU和LRU-S cache的性能差异. 消耗的时间在每一个测试中都被用作性能的衡量标尺.

\subsection{文件服务器}

\indent 图2a 显示了LRU cache在运行完第一个基准测验之后LRU cache的组成(左边的柱). 被写入过的数据块的比例(中间的柱), 还有被读取的数据块的比例(右边的柱). 因为小数点\footnote{作者: 有的不到1\%就记作0了}的原因, cache的柱加在一起不是100\%. 尽管cache活动由于应用程序不同也会各不相同, 这些结果表现了同一个测试标准在一系列不同大小的cache上的表现.
\begin{center}
\includegraphics[scale=0.5]{10.png}

(a) LRU cache和I/O的分解

\includegraphics[scale=0.5]{11.png}

(a) LRU-S cache和I/O的分解

图2: SFS结果. cache组成和数据块读写的分解

\end{center}

\indent 如图所示, LRU cache的占用和读写的数据块很相似. 大部分数据块都是属于大文件的---一个SPECsfs2008中关于文件大小的重言式(大部分文件很小, 但是大部分数据都是在大文件中). 再看一次左侧的柱看起来几乎所有的cache空间都被大文件占据了. 最底下的那一小段表示大小大于64KB的文件, 更小的由于不到1\%就看不见了.

\indent 图2b 显示了LRU-S cache的占用情况. 读写的成分和 图2a 是一样的, 因为我们做的是同一个基准测验, 但是冥想可以看出我们对cache的使用不同. 超过40\%的cache空间都被64KB或者更小的文件使用, 也能看见最底下那元数据了. 不像LRU替换算法, 选择分配和选择收回算法限制了大文件对cache空间的使用. 当使用进行的时候, 大文件的数据块会首先被收回, 空间就留给了小文件和元数据.

\indent 图3a 对比了二者读操作的命中率. 用一个10\%大小的cache, 命中率大概也是10\%. 由于SPECsfs2008工作负荷是随机分布的, 这个结果也在预料之中. 可是, 虽然它们的命中率相同, 可是他们的缺失代价却不同. 在LRU cache的例子中, 大部分的命中都是在大文件上. 在LRU-S cache的例子中, 命中都是在小文件和元数据上. 由于小文件和元数据的查找位置会很随机, 还是在大的连续的文件读取时缺失(未命中)更好.
\begin{center}
\includegraphics[scale=0.5]{12.png}

(a) 读取操作命中率

\includegraphics[scale=0.5]{13.png}

(a) syncer花销

\includegraphics[scale=0.5]{14.png}

(a) SFS性能

图3: SFS性能指数
\end{center}

\indent 图像3b 对比了syncer守护进程的花销(由于cache收回数据块空间造成的数据块搬移比例). 当一个cache单元被收回的时候, syncer必须将数据块从cache中读取, 然后写回到磁盘设备中---这个I/O有可能和应用程序的I/O相干涉. 选择分配算法可以通过当cache空间紧张的时候过滤大的文件减少syncer守护进程的工作. 从而, 我们可以看到由于收回空间导致的I/O所占的比例下降到了3 分之1. 这就说明应用程序工作负荷可以使用更高的性能.

\indent 最后, 图3c 显示了标准测试时候的实际性能. 我们对比了没有cache, 有一个LRU cache, 还有LRU-S cache的. 性能通过运行时间所衡量, 越小就越好. 如图所示, 一个LRU cache只比没有cache好一丁点, 一个LRU-S cache比LRU cache比要快80\%. 用运行时间来衡量, 没有cache的运行了135分钟, 有一个LRU cache的运行了124分钟, 还有LRU-S cache的运行了69分钟.

\indent 很大的性能差异也可以从文件延迟的提升衡量出来. 图4a 和 图4b对比了读和写操作延迟的95\%概率大值, 其中延迟是指写入或读取整个文件所需时间. 横轴代表文件大小(单位是每个SPECsfs2008文件集合)纵轴代表相对于没有cache的情况, 延迟的减少.
\begin{center}
\includegraphics[scale=0.5]{15.png}

(a) 写操作延迟95\%概率大值

\includegraphics[scale=0.5]{16.png}

(a) 读操作延迟95\%概率大值

图4: SFS文件延迟.

\end{center}

\indent 尽管对于很多文件大小(例如: 1KB, 8KB, 256KB, 和512KB), LRU 和LRU-S cache减少的写入延迟大小相同, LRU cache 受到了异常请求的影响, 从而导致了延迟的95\%概率大值的增加. 低于x轴的柱表示LRU cache的写入延迟比没有cache的还要大, 这是由于cache抖动的存在. 读取延迟则显示了LRU-S更多的性能提升. 大小256KB和更小的文件有超过50\%的延迟减少, 比LRU cache那可怜的一点提升强多了. 还有, 我们的cache大小只有10\%, 也就是说只有工作集的10\%能进入cache. LRU-S cache能用10\%的空间缓存小的文件和元数据, 而标准LRU cache将cache空间都浪费在大的连续分布的文件上了. 换种说法, 我们收回大文件所占空间的方法使得更小的文件可以被缓存.

\subsection{电子邮件服务器}

\indent 电子邮件服务器运行工作负荷得到的结果和文件服务器的类似. LRU和LRU-S cache读取操作的cache命中率都是11\%. 再次声明， 由于访问文件的顺序是一个平均随机分布, 所以命中率只和工作集有多大部分可以被缓存有关. 再一次, 二者的缺失代价很不相同. LRU-S可观地减少了读取操作的延迟. 在这个例子中, 32KB大小和更小的文件上可以看出很大程度上延迟的减少. 例如, 2KB 的邮件的读取延迟是85ms, LRU cache能将之减少到21ms, LRU-S cache能减少到4ms(相对于LRU 81\%的减少).

\indent 作为减少缺失代价和减少空间收回消耗(从54\%到25\%)的结果, 电子邮件服务器工作负荷在用LRU-S的时候快了一倍. 没有cache的时候, 一百万个操作用了341分钟, LRU cache用了262分钟, LRU-S cache用了131分钟.

\indent 就如同文件服务器, 一个电子邮件服务器吞吐量也是有限的. 更优先处理元数据和小的电子邮件的话, 可以实现很明显的性能提升. 这个标准测试也同样验证了我们文件系统分类方法的灵活性. 也就是说, 我们的文件大小分类足够处理文件或电子邮件服务器的两种工作负荷. 即使它们有很不同的文件大小分布.

\subsection{文件系统工具}

\indent 文件系统的工具进一步说明了选择性cache的优势. 运行过文件服务器工作负荷之后我们在文件系统中寻找一个不存在的文件(find, 一个100\%只有读取元数据的工作负荷), 创建一个SFS文件系统子目录的磁盘存档, 然后检查文件系统(fsck).

\indent 对于find操作, LRU构造有80\%的命中率, 而LRU-S则有100\%. 于是作为结果, LRU用了48秒, 而LRU-S用了13秒(3.7倍的提速). 对于tar操作, LRU有5\%的命中率, LRU-S则是10\%. 除此之外, LRU接近50\%的I/O都要联系syncer守护进程的活动, 因为在LRU 将tar文件写入cache的时候, 导致要移除cache上的已有单元造成cache抖动. 相比较, LRU-S过滤算法将tar文件的路径指向了磁盘. 结果就是LRU-S用了598秒就完成了档案的创建, 而LRU用了850秒(这是一个42\%的加速).

\indent 最后, LRU用562秒完成了fsck操作, 而LRU-S只用了94秒(一个6倍的提速). 不像LRU, LRU-S无论经过什么测试都将文件系统的元数据保留在cache中, 于是它做文件系统的一致性检查快得多.

\subsection{TPC-H}

\indent 作为一个数据库可以优化I/O的原理上的证明的例子, 我们给文件系统元数据, 用户表, 日志文件, 和临时表最高的优先级; 所有这些类都被当做一个类管理(他们共享一个LRU队列). 索引文件有最低的优先级. 无用的索引会占用很大的一部分cache空间, 而且在这些测试中, 从磁盘读取也足够快了. 我们在分析我们存储系统中数据库的I/O请求的时候发现了这一点. 也就是说, 分类的I/O 既认出了优化cache的机会, 也提供了实现优化的方法.

\indent 图5 对比了LRU和LRU-S的cache组成. 对于LRU测试, 大部分cache都被索引文件占用; 用户表和临时表占据了其他部分. 因为索引文件在数据库创建之后被创建, 可以理解问什么它们针具了cache中这么大的比例. 相比来讲, LRU-S就滤掉了索引文件, 剩下更多的空间给经常被随机访问的用户表使用.
\begin{center}
\includegraphics[scale=0.5]{17.png}

(a) LRU cache和I/O的分解

\includegraphics[scale=0.5]{18.png}

(a) LRU-S cache和I/O的分解

图2: TPC-H结果. cache组成和数据块读写的分解

\end{center}

\indent 最后的结果是cache命中率有所提升(如图6a), 略微减少的cache移除开销(如图6b), 还有查询时间20\%的提升(如图6c). 没有cache运行所有四个查询要680秒, LRU要463秒, LRU-S只要386秒. 而且, 不像文件或电子邮件服务器的运行, 当不采用LRU-S的时候, 我们看到了更不一致的TPC-H运行时间. 这既符合不用cache的情况, 也符合用LRU cache的情况. 于是, 我们用三次运行取平均, 并且引入了误差条. 如 图6c 所示, LRU-S不但运行更快, 而且减少了性能的异常值.
\begin{center}
\includegraphics[scale=0.5]{19.png}

(a) 读取操作命中率

\includegraphics[scale=0.5]{20.png}

(a) syncer花销

\includegraphics[scale=0.5]{21.png}

(a) TPC-H性能

图3: TPC-H性能指数
\end{center}

\section{相关研究}

\indent 文件和储存系统的服务质量是一个被充分研究的领域. 之前的研究集中于保证磁盘I/O的服务质量, 文件系统的服务质量的保证, 配置存储系统以达到性能的要求, 分配存储带宽给应用的类, 还有对应管理员既定的目标以适应存储系统的设计. 相比之下, 我们通过I/O分类去解决服务质量问题, 它通过计算机系统和存储系统的协调获得收益.??

\indent 文件和储存系统的服务质量是一个被充分研究的领域. 之前的研究集中于保证磁盘I/O的服务质量, 文件系统的服务质量的保证, 配置存储系统以达到性能的要求, 分配存储带宽给应用的类, 还有对应管理员既定的目标以适应存储系统的设计. 相比之下, 我们通过I/O分类去解决服务质量问题, 它通过计算机系统和存储系统的协调获得收益.??

\indent 关于cache, 有大量的工作关注于闪存和它作为一个传统的cache进入存储系统. 可是, 因为企业级工作负荷经常展现出很差的访问局部性, 很难让一个传统的cache变得节约且高效.??相比, 我们展示了选择性cache, 即便应用到最简单的cache算法上也能变得效率很高. 即便我们在LRU算法的背景下引入了选择性cache, 任何更高级的算法也可以被应用, 例如LRU-K, CLOCK-Pro, 2Q, ARC, LIRS, FBR, MQ, 和LRFU.

\indent 我们块层的选择性cache的实现很像文件系统层的实现, 比如Conquest和zFS, 他们把更快的存储池留给元数据和小文件. 还有目的类似的其他的块层的cache实现, 只不过实现方式不同. 具体来讲, Hystor的用户数据会被迁移, 比如将元数据和其他的对延迟敏感的数据块移动到更快的存储器上, 而Karma依赖于一个对数据库块访问模式先验的提示, 从而提升多层cache的性能.??

\indent 闪存的特点使得它去做一个持续操作的媒介或者承载一个基于闪存的文件系统都很好. 其他形式的按位可查询地址的稳定的存储也提供了其他文件系统的可能.??

\indent 我们也并不是第一个探索存储系统语义信息的人. 最值得注意的是, 有一种所谓懂语义的磁盘, 还有一种类型安全盘就研究了如何利用磁盘上的数据结构知识去提升性能, 置信度, 和安全性. 但是我们的方法不同, 在于我们伴随I/O请求发送更高等级的语义信息, 而不是通过控制指令发送细节的数据块信息(例如: 内部结构). 进一步地, 不像之前的工作, 我们不会把块管理的工作丢给存储系统.

\section{结论}

\indent 耗费很低的块接口限制了I/O的优化, 有两个原因. 第一, 计算机系统想对复杂的存储系统内部做优化很难. 第二, 存储系统由于缺少语义信息, 想要优化I/O请求也很难.??

\indent 而且, 整个的计算机





















\end{multicols}
\end{document}
